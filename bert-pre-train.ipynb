{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Version 2 + Bug fix - thanks to @chinhuic\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"../input/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# import os\n# print(os.listdir(\"../input/nvidiaapex/repository/NVIDIA-apex-39e153a\"))\n# #print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n# #print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n# #print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\n\n# # Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(action='once')\nimport pickle\n#from apex import amp\nimport shutil","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# %%writefile setup.sh\n\n# git clone https://github.com/NVIDIA/apex\n# pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !sh setup.sh","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#from apex import amp\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda')","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nEPOCHS = 1\nData_dir=\"../input/jigsaw-unintended-bias-in-toxicity-classification\"\nInput_dir = \"../input\"\nWORK_DIR = \"../working/\"\n# num_to_load=1000000                         #Train size to match time limit\nnum_to_load=200000\nvalid_size= 100000                          #Validation Size\nTOXICITY_COLUMN = 'target'","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Add the Bart Pytorch repo to the PATH\n# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\npackage_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5201: ResourceWarning: unclosed file <_io.TextIOWrapper name='/root/.keras/keras.json' mode='r' encoding='UTF-8'>\n  _config = json.load(open(_config_path))\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Translate model from tensorflow to pytorch\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Building PyTorch model from configuration: {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}\n\nConverting TensorFlow checkpoint from /kaggle/input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt\nLoading TF weight bert/embeddings/LayerNorm/beta with shape [768]\nLoading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\nLoading TF weight bert/embeddings/position_embeddings with shape [512, 768]\nLoading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\nLoading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\nLoading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/pooler/dense/bias with shape [768]\nLoading TF weight bert/pooler/dense/kernel with shape [768, 768]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\nLoading TF weight cls/predictions/transform/dense/bias with shape [768]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 768]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../working/pytorch_model.bin\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'../working/bert_config.json'"},"metadata":{}}]},{"cell_type":"code","source":"os.listdir(\"../working\")","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['__notebook_source__.ipynb', 'pytorch_model.bin', 'bert_config.json']"},"metadata":{}}]},{"cell_type":"code","source":"# This is the Bert configuration file\nfrom pytorch_pretrained_bert import BertConfig\n\nbert_config = BertConfig('../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'+'bert_config.json')\n","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Converting the lines to BERT format\n# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"t_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"t_df.shape","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(1804874, 45)"},"metadata":{}}]},{"cell_type":"code","source":"t_df.head()","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n2  59852            ...                                    4\n3  59855            ...                                    4\n4  59856            ...                                   47\n\n[5 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.000000</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.000000</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59852</td>\n      <td>0.000000</td>\n      <td>This is such an urgent design problem; kudos t...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:45.222647+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59855</td>\n      <td>0.000000</td>\n      <td>Is this something I'll be able to install on m...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:47.601894+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.0</td>\n      <td>0.021277</td>\n      <td>0.87234</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015-09-29 10:50:48.488476+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\ntrain_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).sample(num_to_load+valid_size,random_state=SEED)\nprint('loaded %d records' % len(train_df))\n\n# Make sure all comment_text values are strings\ntrain_df['comment_text'] = train_df['comment_text'].astype(str) \n\nsequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\ntrain_df=train_df.fillna(0)\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\ny_columns=['target']\n\ntrain_df = train_df.drop(['comment_text'],axis=1)\n# convert target to 0,1\ntrain_df['target']=(train_df['target']>=0.5).astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"loaded 300000 records\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=300000), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f31ea7db4ab416ba9d0b4b8348d4a63"}},"metadata":{}}]},{"cell_type":"code","source":"print(\"H\")","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"H\n","output_type":"stream"}]},{"cell_type":"code","source":"\nX = sequences[:num_to_load]                \ny = train_df[y_columns].values[:num_to_load]\nX_val = sequences[num_to_load:]                \ny_val = train_df[y_columns].values[num_to_load:]\n","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"test_df=train_df.tail(valid_size).copy()\ntrain_df=train_df.head(num_to_load)","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"              id            ...             toxicity_annotator_count\n1486300  5939050            ...                                    4\n748327   5037085            ...                                    4\n1658806  6155207            ...                                    4\n211594    500973            ...                                    4\n1001776  5342605            ...                                    4\n\n[5 rows x 44 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1486300</th>\n      <td>5939050</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2017-09-12 15:24:15.700688+00</td>\n      <td>102</td>\n      <td>0.0</td>\n      <td>376753</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>748327</th>\n      <td>5037085</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2017-03-22 07:10:55.030527+00</td>\n      <td>54</td>\n      <td>0.0</td>\n      <td>320990</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1658806</th>\n      <td>6155207</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2017-10-16 12:23:27.012280+00</td>\n      <td>102</td>\n      <td>6151988.0</td>\n      <td>389313</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>211594</th>\n      <td>500973</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2016-10-01 18:05:43.812454+00</td>\n      <td>54</td>\n      <td>0.0</td>\n      <td>147285</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1001776</th>\n      <td>5342605</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2017-06-01 23:18:38.015699+00</td>\n      <td>54</td>\n      <td>5339342.0</td>\n      <td>339640</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_dataset.tensors","metadata":{"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(tensor([[  101,  2009,  1005,  ...,     0,     0,     0],\n         [  101,  2016,  3858,  ...,     0,     0,     0],\n         [  101,  6522, 10536,  ...,  2123,  1005,   102],\n         ...,\n         [  101,  2051,  2000,  ...,     0,     0,     0],\n         [  101,  7632,  1010,  ...,     0,     0,     0],\n         [  101,  2001,  2009,  ...,     0,     0,     0]]), tensor([[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]))"},"metadata":{}}]},{"cell_type":"code","source":"#!git clone https://github.com/NVIDIA/apex","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cd apex\n# !pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" apex/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from apex import amp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_model_file = \"bert_pytorch.bin\"\n\nlr=2e-5\nbatch_size = 32\naccumulation_steps=2\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nmodel = BertForSequenceClassification.from_pretrained(\"../working\",cache_dir=None,num_labels=len(y_columns))\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\n#model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel=model.train()\n\ntq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n    for i,(x_batch, y_batch) in tk0:\n#        optimizer.zero_grad()\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n#         with amp.scale_loss(loss, optimizer) as scaled_loss:\n#             scaled_loss.backward()\n        loss.backward()\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.step()                            # Now we can do an optimizer step\n            optimizer.zero_grad()\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss = lossf)\n        avg_loss += loss.item() / len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n\n\ntorch.save(model.state_dict(), output_model_file)\n","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7fe0aec50df0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fabc6447f10c49db8bfef2a0626fc97d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run validation\n# The following 2 lines are not needed but show how to download the model for prediction\nmodel = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\nmodel.load_state_dict(torch.load(output_model_file ))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.eval()\nvalid_preds = np.zeros((len(X_val)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n\n\n","metadata":{"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=3125), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d4896b648043b6929b741f667fea7f"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# From baseline kernel\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]>0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>0.5]\n    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\nMODEL_NAME = 'model1'\ntest_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\nbias_metrics_df\nget_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))","metadata":{"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"   bnsp_auc  bpsn_auc      ...       subgroup_auc  subgroup_size\n7  0.976673  0.820185      ...           0.840527           1320\n6  0.979236  0.806438      ...           0.845762            776\n2  0.976298  0.821443      ...           0.847479            540\n5  0.967946  0.874415      ...           0.871570           1079\n4  0.962222  0.916405      ...           0.902338            391\n1  0.963684  0.930744      ...           0.923033           2792\n0  0.969644  0.921460      ...           0.924610           2188\n3  0.951837  0.950620      ...           0.928802           1895\n8  0.977654  0.916082      ...           0.941077            252\n\n[9 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bnsp_auc</th>\n      <th>bpsn_auc</th>\n      <th>subgroup</th>\n      <th>subgroup_auc</th>\n      <th>subgroup_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>0.976673</td>\n      <td>0.820185</td>\n      <td>white</td>\n      <td>0.840527</td>\n      <td>1320</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.979236</td>\n      <td>0.806438</td>\n      <td>black</td>\n      <td>0.845762</td>\n      <td>776</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.976298</td>\n      <td>0.821443</td>\n      <td>homosexual_gay_or_lesbian</td>\n      <td>0.847479</td>\n      <td>540</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.967946</td>\n      <td>0.874415</td>\n      <td>muslim</td>\n      <td>0.871570</td>\n      <td>1079</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.962222</td>\n      <td>0.916405</td>\n      <td>jewish</td>\n      <td>0.902338</td>\n      <td>391</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.963684</td>\n      <td>0.930744</td>\n      <td>female</td>\n      <td>0.923033</td>\n      <td>2792</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.969644</td>\n      <td>0.921460</td>\n      <td>male</td>\n      <td>0.924610</td>\n      <td>2188</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.951837</td>\n      <td>0.950620</td>\n      <td>christian</td>\n      <td>0.928802</td>\n      <td>1895</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.977654</td>\n      <td>0.916082</td>\n      <td>psychiatric_or_mental_illness</td>\n      <td>0.941077</td>\n      <td>252</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.924082299510814"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}