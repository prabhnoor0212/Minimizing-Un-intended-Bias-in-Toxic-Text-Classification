{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "LSTM_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "gjJIh9Qmba5K",
        "outputId": "a342ff5d-2456-4504-92cd-552d803bed9e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import random\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
        "from keras.preprocessing import text, sequence\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rXtyPHgPba5L"
      },
      "source": [
        "# disable progress bars when submitting\n",
        "def is_interactive():\n",
        "   return 'SHLVL' not in os.environ\n",
        "\n",
        "if not is_interactive():\n",
        "    def nop(it, *a, **k):\n",
        "        return it\n",
        "\n",
        "    tqdm = nop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vZMm83Siba5L"
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DreIUWhSba5M"
      },
      "source": [
        "CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
        "GLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
        "NUM_MODELS = 1\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "MAX_LEN = 220"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "anEy6541ba5M"
      },
      "source": [
        "max_features=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "Ln4Rk-Maba5N"
      },
      "source": [
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path) as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    embedding_index = load_embeddings(path)\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    unknown_words = []\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            unknown_words.append(word)\n",
        "    return embedding_matrix, unknown_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPO_eqLfba5O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rEzqOXDkba5O"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
        "                batch_size=512, n_epochs=4,\n",
        "                enable_checkpoint_ensemble=True):\n",
        "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
        "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "    all_test_preds = []\n",
        "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        model.train()\n",
        "        avg_loss = 0.\n",
        "        \n",
        "        for data in tqdm(train_loader, disable=False):\n",
        "            x_batch = data[:-1]\n",
        "            y_batch = data[-1]\n",
        "\n",
        "            y_pred = model(*x_batch)            \n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item() / len(train_loader)\n",
        "            \n",
        "        model.eval()\n",
        "        test_preds = np.zeros((len(test), output_dim))\n",
        "    \n",
        "        for i, x_batch in enumerate(test_loader):\n",
        "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
        "\n",
        "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
        "\n",
        "        all_test_preds.append(test_preds)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
        "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
        "\n",
        "    if enable_checkpoint_ensemble:\n",
        "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
        "    else:\n",
        "        test_preds = all_test_preds[-1]\n",
        "        \n",
        "    return test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bhe8k32Dba5S"
      },
      "source": [
        "class SpatialDropout(nn.Dropout2d):\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "        x = x.squeeze(2)  # (N, T, K)\n",
        "        return x\n",
        "    \n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_aux_targets):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        embed_size = embedding_matrix.shape[1]\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.embedding_dropout = SpatialDropout(0.3)\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "        #self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "    \n",
        "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
        "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
        "        \n",
        "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
        "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = self.embedding_dropout(h_embedding)\n",
        "        \n",
        "        h_lstm1, _ = self.lstm1(h_embedding)\n",
        "        #h_lstm2, _ = self.lstm2(h_lstm1)\n",
        "        \n",
        "        # global average pooling\n",
        "        avg_pool = torch.mean(h_lstm1, 1)\n",
        "        # global max pooling\n",
        "        max_pool, _ = torch.max(h_lstm1, 1)\n",
        "        \n",
        "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
        "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
        "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
        "        \n",
        "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
        "        \n",
        "        result = self.linear_out(hidden)\n",
        "        aux_result = self.linear_aux_out(hidden)\n",
        "        out = torch.cat([result, aux_result], 1)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JApgFMnWba5T"
      },
      "source": [
        "def preprocess(data):\n",
        "    '''\n",
        "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
        "    '''\n",
        "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "    def clean_special_chars(text, punct):\n",
        "        for p in punct:\n",
        "            text = text.replace(p, ' ')\n",
        "        return text\n",
        "\n",
        "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwaoFvpaba5U"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "loU2S26-ba5U"
      },
      "source": [
        "valid_size = 100000\n",
        "num_to_load = 200000\n",
        "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
        "#test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
        "\n",
        "test = train.tail(valid_size).copy()\n",
        "train=train.head(num_to_load)\n",
        "\n",
        "x_train = preprocess(train['comment_text'])\n",
        "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
        "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
        "#x_test = preprocess(test['comment_text'])\n",
        "\n",
        "x_test = preprocess(test['comment_text'])\n",
        "y_test = np.where(test['target'] >= 0.5, 1, 0)\n",
        "y_aux_test = test[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "57kaDKzyba5W",
        "outputId": "28be1676-b252-4593-bbe7-eb87389faf75"
      },
      "source": [
        "x_train.shape, y_train.shape, y_aux_train.shape, x_test.shape, y_test.shape, y_aux_test.shape, "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((200000,), (200000,), (200000, 6), (100000,), (100000,), (100000, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "raQS55Dkba5X",
        "outputId": "2a61081a-bf42-4349-cc6d-b2993672b4e3"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         This is so cool  It s like   would you want yo...\n",
              "1         Thank you   This would make my life a lot less...\n",
              "2         This is such an urgent design problem  kudos t...\n",
              "3         Is this something I ll be able to install on m...\n",
              "4                      haha you guys are a bunch of losers \n",
              "5                                      ur a sh tty comment \n",
              "6                               hahahahahahahahhha suck it \n",
              "7                                       FFFFUUUUUUUUUUUUUUU\n",
              "8         The ranchers seem motivated by mostly by greed...\n",
              "9         It was a great show  Not a combo I d of expect...\n",
              "10                                  Wow  that sounds great \n",
              "11        This is a great story  Man  I wonder if the pe...\n",
              "12           This seems like a step in the right direction \n",
              "13        It s ridiculous that these guys are being call...\n",
              "14        This story gets more ridiculous by the hour  A...\n",
              "15        I agree  I don t want to grant them the legiti...\n",
              "16        Interesting  I ll be curious to see how this w...\n",
              "17                          Awesome  I love Civil Comments \n",
              "18        I m glad you re working on this  and I look fo...\n",
              "19        Angry trolls  misogynists and Racists   oh my ...\n",
              "20        Nice to some attempts to try to make comments ...\n",
              "21        One would hope that the purpose of introducing...\n",
              "22        Comments will be randomly chosen and be review...\n",
              "23        She would be a major improvement for city coun...\n",
              "24        I agree  Comments have so much potential to be...\n",
              "25        Great question  It s one we re asked a lot  We...\n",
              "26        Thanks  Christa   Will you be adding any featu...\n",
              "27        Our aim is actually the opposite  we want spir...\n",
              "28        Thanks  We re really going to try   not only t...\n",
              "29        I applaud Civil s efforts to create some new t...\n",
              "                                ...                        \n",
              "199970     If land desecration is such an issue  CLEAN U...\n",
              "199971    Imagine how much better life would be for the ...\n",
              "199972                    Thank you for the quick response \n",
              "199973    Call You Out as a trader and a paid spook for ...\n",
              "199974    All Convicted felons in halfway housed have a ...\n",
              "199975    Your reply is spot on  the inmates are scored ...\n",
              "199976    Like you say   you can t be that stupid   I th...\n",
              "199977    I wish to believe TMT would provide the STEM j...\n",
              "199978    Brad  I have huge issues with anyone who threa...\n",
              "199979    domestic \\n\\nThe reference is in regards to th...\n",
              "199980    Sorry Mr  Cooper  of the 330 that participated...\n",
              "199981                                 People go to supper \n",
              "199982    Thanks Andrew  I had to go all of the way to t...\n",
              "199983    The oil tax credits have to taken away from th...\n",
              "199984                            UNCONSTITUTIONAL   RACIST\n",
              "199985    Do some more coke Craig and Bill  You two are ...\n",
              "199986    Capitan   that investment in Virginia is a rea...\n",
              "199987    Ground control to Major Chong  \\n\\nTake your p...\n",
              "199988                    Is Yukon Gold airing this season \n",
              "199989    I went to one of Provos Sunday church services...\n",
              "199990                            Tarragon is tarrogent    \n",
              "199991    That s nice seeing how the CEO made  26 millio...\n",
              "199992    Let me guess  another entitled baby boomer tha...\n",
              "199993    Glad the survey results were revealed PRIOR to...\n",
              "199994    Frank  they need you at the visitors and conve...\n",
              "199995    Swooned like no other  ever  Except of course ...\n",
              "199996    It takes a big man to admit he was wrong  Mr  ...\n",
              "199997                                   Cal meant Putin   \n",
              "199998    Guaranteed  if you elect Metcalfe this year an...\n",
              "199999    I m guessing you don t know West High   It s c...\n",
              "Name: comment_text, Length: 200000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oD7fvdvzba5X"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9GYh1DVZba5Y",
        "outputId": "eb208c48-57ea-4569-d022-5a4dbb8dd61a"
      },
      "source": [
        "max_features = max_features or len(tokenizer.word_index) + 1\n",
        "max_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120755"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "30ucnPXjba5Z",
        "outputId": "0d655a22-7242-4a93-ba56-1f21a0b1236d"
      },
      "source": [
        "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
        "print('n unknown words (crawl): ', len(unknown_words_crawl))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n unknown words (crawl):  36875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "03tN0BkZba5a",
        "outputId": "da53e279-460d-407a-c2f6-ab507842318d"
      },
      "source": [
        "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
        "print('n unknown words (glove): ', len(unknown_words_glove))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n unknown words (glove):  35435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XqWe1kj9ba5a",
        "outputId": "0c65b70b-64c8-421f-8f1d-894e70d049bf"
      },
      "source": [
        "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
        "embedding_matrix.shape\n",
        "\n",
        "del crawl_matrix\n",
        "del glove_matrix\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "od5x9ym_ba5c"
      },
      "source": [
        "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
        "x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
        "y_train_torch= torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K6-TzZJ0ba5c"
      },
      "source": [
        "y_train_torch.shape[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGrl87Cbba5d"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SEkRwQZVba5d",
        "outputId": "5bc1ef66-7ad2-45bd-b870-d842ed0d9345"
      },
      "source": [
        "train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
        "test_dataset = data.TensorDataset(x_test_torch)\n",
        "\n",
        "all_test_preds = []\n",
        "\n",
        "for model_idx in range(NUM_MODELS):\n",
        "    print('Model ', model_idx)\n",
        "    seed_everything(1234 + model_idx)\n",
        "    \n",
        "    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
        "    model.cuda()\n",
        "    \n",
        "    test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n",
        "                             loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
        "    all_test_preds.append(test_preds)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model  0\n",
            "Epoch 1/4 \t loss=0.1311 \t time=49.89s\n",
            "Epoch 2/4 \t loss=0.1076 \t time=49.53s\n",
            "Epoch 3/4 \t loss=0.1044 \t time=49.50s\n",
            "Epoch 4/4 \t loss=0.1029 \t time=49.67s\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WwnXelB9ba5e"
      },
      "source": [
        "submission = pd.DataFrame.from_dict({\n",
        "    'id': test['id'],\n",
        "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bdfrnx5Zba5f"
      },
      "source": [
        "test['model1'] = submission['prediction']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "81g5mIuDba5f"
      },
      "source": [
        "# From baseline kernel\n",
        "\n",
        "def calculate_overall_auc(df, model_name):\n",
        "    true_labels = df[TOXICITY_COLUMN]>0.5\n",
        "    predicted_labels = df[model_name]\n",
        "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
        "\n",
        "def power_mean(series, p):\n",
        "    total = sum(np.power(series, p))\n",
        "    return np.power(total / len(series), 1 / p)\n",
        "\n",
        "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
        "    bias_score = np.average([\n",
        "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
        "        power_mean(bias_df[BPSN_AUC], POWER),\n",
        "        power_mean(bias_df[BNSP_AUC], POWER)\n",
        "    ])\n",
        "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
        "\n",
        "\n",
        "\n",
        "SUBGROUP_AUC = 'subgroup_auc'\n",
        "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
        "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    try:\n",
        "        return metrics.roc_auc_score(y_true, y_pred)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
        "    subgroup_examples = df[df[subgroup]>0.5]\n",
        "    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n",
        "\n",
        "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
        "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
        "    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n",
        "    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n",
        "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
        "    return compute_auc(examples[label]>0.5, examples[model_name])\n",
        "\n",
        "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
        "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
        "    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n",
        "    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n",
        "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
        "    return compute_auc(examples[label]>0.5, examples[model_name])\n",
        "\n",
        "def compute_bias_metrics_for_model(dataset,\n",
        "                                   subgroups,\n",
        "                                   model,\n",
        "                                   label_col,\n",
        "                                   include_asegs=False):\n",
        "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
        "    records = []\n",
        "    for subgroup in subgroups:\n",
        "        record = {\n",
        "            'subgroup': subgroup,\n",
        "            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n",
        "        }\n",
        "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
        "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
        "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
        "        records.append(record)\n",
        "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AjNbmmSIba5g"
      },
      "source": [
        "identity_columns = [\n",
        "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
        "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ni2MofU5ba5g",
        "outputId": "2f930ab5-301b-40ce-ff9e-29207f0be3c3"
      },
      "source": [
        "from sklearn import metrics\n",
        "MODEL_NAME = 'model1'\n",
        "test[MODEL_NAME]=torch.sigmoid(torch.tensor(test[MODEL_NAME].values)).numpy()\n",
        "TOXICITY_COLUMN = 'target'\n",
        "bias_metrics_df = compute_bias_metrics_for_model(test, identity_columns, MODEL_NAME, 'target')\n",
        "bias_metrics_df\n",
        "get_final_metric(bias_metrics_df, calculate_overall_auc(test, MODEL_NAME))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9213733808580543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6oWxoQVdba5g",
        "outputId": "c0365085-cc25-4f8e-e933-08927efbb6bf"
      },
      "source": [
        "bias_metrics_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   bnsp_auc  bpsn_auc      ...       subgroup_auc  subgroup_size\n",
              "6  0.947711  0.847913      ...           0.825643            759\n",
              "2  0.959777  0.823556      ...           0.843644            735\n",
              "7  0.958968  0.838491      ...           0.847883           1389\n",
              "5  0.951116  0.880010      ...           0.882232            814\n",
              "4  0.951376  0.913129      ...           0.921034            277\n",
              "0  0.953492  0.916610      ...           0.922338           2560\n",
              "1  0.944865  0.931586      ...           0.923710           3501\n",
              "8  0.957935  0.904934      ...           0.925611            315\n",
              "3  0.952094  0.938236      ...           0.944642           1896\n",
              "\n",
              "[9 rows x 5 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bnsp_auc</th>\n",
              "      <th>bpsn_auc</th>\n",
              "      <th>subgroup</th>\n",
              "      <th>subgroup_auc</th>\n",
              "      <th>subgroup_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.947711</td>\n",
              "      <td>0.847913</td>\n",
              "      <td>black</td>\n",
              "      <td>0.825643</td>\n",
              "      <td>759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.959777</td>\n",
              "      <td>0.823556</td>\n",
              "      <td>homosexual_gay_or_lesbian</td>\n",
              "      <td>0.843644</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.958968</td>\n",
              "      <td>0.838491</td>\n",
              "      <td>white</td>\n",
              "      <td>0.847883</td>\n",
              "      <td>1389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.951116</td>\n",
              "      <td>0.880010</td>\n",
              "      <td>muslim</td>\n",
              "      <td>0.882232</td>\n",
              "      <td>814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.951376</td>\n",
              "      <td>0.913129</td>\n",
              "      <td>jewish</td>\n",
              "      <td>0.921034</td>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.953492</td>\n",
              "      <td>0.916610</td>\n",
              "      <td>male</td>\n",
              "      <td>0.922338</td>\n",
              "      <td>2560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.944865</td>\n",
              "      <td>0.931586</td>\n",
              "      <td>female</td>\n",
              "      <td>0.923710</td>\n",
              "      <td>3501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.957935</td>\n",
              "      <td>0.904934</td>\n",
              "      <td>psychiatric_or_mental_illness</td>\n",
              "      <td>0.925611</td>\n",
              "      <td>315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.952094</td>\n",
              "      <td>0.938236</td>\n",
              "      <td>christian</td>\n",
              "      <td>0.944642</td>\n",
              "      <td>1896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}